{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercicio 1: crie uma função de ML_pipeline\n",
    "\n",
    "Nela tem como parametros: dados de treino, dados de teste, modelo, tipo de treinamento(mediante um parametro string) que diz 'kfold' , 'stratifiedkfold' , 'groupstratkfold', número de folhas, metodo de avaliação, qual score(loss function)\n",
    "\n",
    "tem como objetivo algo importante como\n",
    "\n",
    "Treinamento, fixando o tipo de treino: k fold, leave one out, group k fold, stratified group k fold\n",
    "\n",
    "\n",
    "Ela retorna\n",
    "\n",
    "model, resultado nos dados de treino em cada folha(lista), resultado nos dados de teste(lista, em cada folha tbm), valor médio nos dados de treino, valor médio nos dados de teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_pct_split = 0.3\n",
    "data_set = pd.read_csv('./heart_failure_clinical_records.csv')\n",
    "test = data_set.sample(frac = train_test_pct_split)\n",
    "train = data_set.loc[~data_set.index.isin(test.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convensão coluna target esta sempre ao final do df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML_pipeline():\n",
    "    \"\"\"Nessa Classe vc pode contruir um objeto que tem as propriedades de treinar\n",
    "    um modelo utilizando diversas tecnicas de cross-validation incluindo:\n",
    "    -> k_fold \\n\n",
    "    -> stratified_kfold \\n\n",
    "    -> leave_one_out_kfold \\n\n",
    "    -> group_kfold \\n\n",
    "    -> groupstrat_kfold \\n\n",
    "\n",
    "    É esperado que a coluna target seja a ultima coluna do dataframe de treino\n",
    "    \"\"\"\n",
    "    def __init__(self,n,data):\n",
    "        self.n = n\n",
    "        self.data = data\n",
    "\n",
    "    def k_fold(self,model,score):\n",
    "        \"\"\"Essa função gera os k fold treinamentos para modelo.\n",
    "        Init: O df é aleatoriamente dividido e é determinado a quantidade de amostras no dado de teste\n",
    "        Para cada iteração as partes são dividas em treino e teste.\n",
    "        Todo o dado é utilizado percorrendo em conjuntos de partes\n",
    "\n",
    "        Args:\n",
    "            n (int): numero de folds do k_fold\n",
    "            data (pd.Dataframe): dataframe com coluna resposta ao final das de sua coluna\n",
    "            model (sklearn_mdoel): modelo escolhido do pacote sklearn\n",
    "            loss_function (sklearn_loss_function): score utilizado\n",
    "\n",
    "        Returns:\n",
    "            dict: dicionário com melhor modelo,\n",
    "            resultado para os n folds (teste e treino) e média do score (teste e treino) \n",
    "        \"\"\"\n",
    "        df = self.data.sample(n=len(self.data)).copy()\n",
    "        partes_iguais = math.ceil(len(df)/self.n) \n",
    "        folds = {'score_treino': [],\n",
    "                'score_teste':[]}\n",
    "        trained_model = model\n",
    "        modelos = []\n",
    "        for i in tqdm(range(1,self.n+1)):\n",
    "            test = df.iloc[partes_iguais*(i-1):partes_iguais*i]\n",
    "            train = df.loc[~df.index.isin(test.index)]\n",
    "\n",
    "            X_train = train[df.columns[:-1]]\n",
    "            y_train = train[df.columns[-1]]\n",
    "\n",
    "            X_test = test[df.columns[:-1]]\n",
    "            y_test = test[df.columns[-1]]\n",
    "\n",
    "            \n",
    "            trained_model = trained_model.fit(X_train,y_train)\n",
    "            pred_train = trained_model.predict(X_train)\n",
    "            pred_test = trained_model.predict(X_test)\n",
    "\n",
    "            score_treino = score(y_train,pred_train)\n",
    "            score_teste = score(y_test,pred_test)\n",
    "\n",
    "            modelos.append(trained_model)\n",
    "\n",
    "            folds['score_teste'].append(score_teste)\n",
    "            folds['score_treino'].append(score_treino)\n",
    "            folds['melhor_modelo'] = modelos[np.argmax(folds['score_teste'])]\n",
    "\n",
    "            folds['media_treino'] = np.mean(folds['score_treino'])\n",
    "            folds['media_teste'] = np.mean(folds['score_teste'])\n",
    "        \n",
    "\n",
    "        return folds\n",
    "    \n",
    "    def stratified_kfold(self,model,loss_function):\n",
    "        \"\"\"Essa função gera os kfold dados de treinamento mantendo a proporção entre as N classes para \n",
    "        cada agrupamento de treinamento e teste.\n",
    "\n",
    "        Args:\n",
    "            n (int): numero de folds\n",
    "            data (pd.DataFrame): dataframe com coluna resposta ao final das de sua coluna\n",
    "            model (sklearn_model): modelo escolhido do pacote sklearn\n",
    "            loss_function (sklearn_loss_function): score utilizado\n",
    "\n",
    "        Returns:\n",
    "            dict: dicionário com melhor modelo,\n",
    "            resultado para os n folds (teste e treino) e média do score (teste e treino) \n",
    "        \"\"\"\n",
    "        df = self.data.sample(n=len(self.data)).copy()\n",
    "        folds = {'score_treino': [],\n",
    "                'score_teste':[]}\n",
    "        modelos = []\n",
    "        grupos_de_classes = df.groupby(df.columns[-1])\n",
    "        proporcao = grupos_de_classes.apply(len)/len(df)\n",
    "        partes_iguais_proporcao = (len(df)/self.n * proporcao).apply(math.ceil)\n",
    "\n",
    "        for i in tqdm(range(1,self.n+1)):\n",
    "            classes = {'train':[],\n",
    "                    'test':[]}\n",
    "            for k,v in grupos_de_classes:\n",
    "                test_class = v.iloc[partes_iguais_proporcao[k]*(i-1):partes_iguais_proporcao[k]*i] # garantia de manter proporcao em dados de teste\n",
    "                train_class = v.loc[~v.index.isin(test_class.index)] # garantia de manter a proporcao em dados de treino\n",
    "                classes['train'].append(train_class)\n",
    "                classes['test'].append(test_class)\n",
    "\n",
    "            train = pd.concat(classes['train'])\n",
    "            test = pd.concat(classes['test'])\n",
    "\n",
    "            X_train = train[df.columns[:-1]]\n",
    "            y_train = train[df.columns[-1]]\n",
    "\n",
    "            X_test = test[df.columns[:-1]]\n",
    "            y_test = test[df.columns[-1]]\n",
    "\n",
    "            \n",
    "            model = model.fit(X_train,y_train)\n",
    "            pred_train = model.predict(X_train)\n",
    "            pred_test = model.predict(X_test)\n",
    "\n",
    "            score_treino = loss_function(y_train,pred_train)\n",
    "            score_teste = loss_function(y_test,pred_test)\n",
    "\n",
    "            modelos.append(model)\n",
    "\n",
    "            folds['score_teste'].append(score_teste)\n",
    "            folds['score_treino'].append(score_treino)\n",
    "            folds['melhor_modelo'] = modelos[np.argmax(folds['score_teste'])]\n",
    "\n",
    "            folds['media_treino'] = np.mean(folds['score_treino'])\n",
    "            folds['media_teste'] = np.mean(folds['score_teste'])\n",
    "\n",
    "        return folds\n",
    "    \n",
    "    def leave_one_out_kfold(self,model,loss_function)->dict:\n",
    "        \"\"\"Essa função gera os k fold treinamentos para modelo.\n",
    "        Init: O df é aleatoriamente dividido e é determinado a quantidade de amostras no dado de teste\n",
    "        Para cada iteração as partes são dividas em treino e teste.\n",
    "        Todo o dado é utilizado percorrendo em conjuntos de partes\n",
    "\n",
    "        Args:\n",
    "            n (int): numero de folds do k_fold\n",
    "            data (pd.Dataframe): dataframe com coluna resposta ao final das de sua coluna\n",
    "            model (sklearn_mdoel): modelo escolhido do pacote sklearn\n",
    "            loss_function (sklearn_loss_function): score utilizado\n",
    "\n",
    "        Returns:\n",
    "            dict: dicionário com melhor modelo,\n",
    "            resultado para os n folds (teste e treino) e média do score (teste e treino) \n",
    "        \"\"\"\n",
    "        df = self.data.sample(n=len(self.data)).copy()\n",
    "        n = len(self.data)\n",
    "        partes_iguais = math.ceil(len(df)/n)\n",
    "        folds = {'score_treino': [],\n",
    "                'score_teste':[]}\n",
    "        modelos = []\n",
    "        for i in tqdm(range(1,n+1)):\n",
    "            test = df.iloc[partes_iguais*(i-1):partes_iguais*i]\n",
    "            train = df.loc[~df.index.isin(test.index)]\n",
    "\n",
    "            X_train = train[df.columns[:-1]]\n",
    "            y_train = train[df.columns[-1]]\n",
    "\n",
    "            X_test = test[df.columns[:-1]]\n",
    "            y_test = test[df.columns[-1]]\n",
    "\n",
    "            \n",
    "            model = model.fit(X_train,y_train)\n",
    "            pred_train = model.predict(X_train)\n",
    "            pred_test = model.predict(X_test)\n",
    "\n",
    "            score_treino = loss_function(y_train,pred_train)\n",
    "            score_teste = loss_function(y_test,pred_test)\n",
    "\n",
    "            modelos.append(model)\n",
    "\n",
    "            folds['score_teste'].append(score_teste)\n",
    "            folds['score_treino'].append(score_treino)\n",
    "            folds['melhor_modelo'] = modelos[np.argmax(folds['score_teste'])]\n",
    "\n",
    "            folds['media_treino'] = np.mean(folds['score_treino'])\n",
    "            folds['media_teste'] = np.mean(folds['score_teste'])\n",
    "\n",
    "        return folds\n",
    "    \n",
    "    def group_kfold(self,group_col:str,model,loss_function):\n",
    "        \"\"\"essa funcao crie n folds para cada grupo exclusivo. Isolando o treinamento apenas ao conjunto de dados complementar ao grupo\n",
    "\n",
    "        Args:\n",
    "            n (int): numero de folds por grupo\n",
    "            group_col (str): coluna do grupo\n",
    "            data (pd.DataFrame): dados de treino\n",
    "            model (_type_): modelo\n",
    "            loss_function (_type_): score\n",
    "\n",
    "        Returns:\n",
    "            dict: dicionario com informações do modelo\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.data.sample(n=len(self.data)).copy()\n",
    "        folds = {'score_treino': [],\n",
    "                'score_teste':[]}\n",
    "        modelos = []\n",
    "        grupos_de_classes = df.groupby(group_col)\n",
    "\n",
    "        \n",
    "        for _, v in tqdm(grupos_de_classes): # para cada grupo é feito o treinamento excluindo o grupo em n partes iguais\n",
    "            test = v\n",
    "            train = df.loc[~df.index.isin(test.index)] # sempre treinar os dados no conjunto de dados complementares ao grupo\n",
    "\n",
    "            partes_iguais_test = math.ceil(len(test)/self.n)\n",
    "            #partes_iguais_treino = math.ceil(len(train)/n)\n",
    "            \n",
    "            for i in range(1,self.n+1):\n",
    "                test_fold = test.iloc[partes_iguais_test*(i-1):partes_iguais_test*i] # testar o modelo \n",
    "                #train_fold = train.iloc[partes_iguais_treino*(i-1):partes_iguais_treino*i] # talvez se quiser treinar o modelo em conjuntos do grupo complementar\n",
    "\n",
    "                X_train = train[df.columns[:-1]]\n",
    "                y_train = train[df.columns[-1]]\n",
    "                X_test = test_fold[df.columns[:-1]]\n",
    "                y_test = test_fold[df.columns[-1]]\n",
    "\n",
    "                \n",
    "                model = model.fit(X_train,y_train)\n",
    "                pred_train = model.predict(X_train)\n",
    "                pred_test = model.predict(X_test)\n",
    "\n",
    "                score_treino = loss_function(y_train,pred_train)\n",
    "                score_teste = loss_function(y_test,pred_test)\n",
    "\n",
    "                modelos.append(model)\n",
    "\n",
    "                folds['score_teste'].append(score_teste)\n",
    "                folds['score_treino'].append(score_treino)\n",
    "\n",
    "                folds['melhor_modelo'] = modelos[np.argmax(folds['score_teste'])]\n",
    "                folds['media_treino'] = np.mean(folds['score_treino'])\n",
    "                folds['media_teste'] = np.mean(folds['score_teste'])\n",
    "\n",
    "        return folds\n",
    "    \n",
    "    def groupstrat_kfold(self,group_col:str,model,loss_function):\n",
    "        \"\"\"essa funcao crie n folds para cada grupo exclusivo. Isolando o treinamento apenas ao conjunto de dados complementar ao grupo respeitando\n",
    "        a propocao de dados no conjunto de treino\n",
    "\n",
    "        Args:\n",
    "            n (int): numero de folds por grupo\n",
    "            group_col (str): coluna do grupo\n",
    "            data (pd.DataFrame): dados de treino\n",
    "            model (_type_): modelo\n",
    "            loss_function (_type_): score\n",
    "\n",
    "        Returns:\n",
    "            dict: dicionario com informações do modelo\n",
    "        \"\"\"\n",
    "\n",
    "        df = self.data.sample(n=len(self.data)).copy()\n",
    "        target_column = df.columns[-1]\n",
    "        folds = {'score_treino': [],\n",
    "                    'score_teste':[]}\n",
    "        modelos = []\n",
    "        grupos_de_classes = df.groupby(group_col)\n",
    "        proporcao_target = df[target_column].value_counts()/len(df)\n",
    "\n",
    "        for label_grupo, grupo in tqdm(grupos_de_classes): # para cada grupo é feito o treinamento excluindo o grupo em n partes iguais\n",
    "\n",
    "            teste_ = grupo\n",
    "            train_ = df.loc[~df.index.isin(grupo.index)]\n",
    "\n",
    "            partes_iguais_test = ((len(teste_)/self.n)*proporcao_target).apply(math.ceil)\n",
    "            partes_iguais_treino = ((len(train_)/self.n)*proporcao_target).apply(math.ceil)\n",
    "\n",
    "            for i in range(1,self.n+1):\n",
    "                classes = {'train':[],\n",
    "                            'test':[]}\n",
    "                \n",
    "                for k in proporcao_target.index:\n",
    "                    test_class = teste_.iloc[partes_iguais_test[k]*(i-1):partes_iguais_test[k]*i] # garantia de manter proporcao em dados de teste\n",
    "                    train_class = train_.iloc[partes_iguais_treino[k]*(i-1):partes_iguais_treino[k]*i] # garantia de manter a proporcao em dados de treino\n",
    "\n",
    "\n",
    "                    classes['train'].append(train_class)\n",
    "                    classes['test'].append(test_class)\n",
    "\n",
    "\n",
    "                test_fold = pd.concat(classes['test'])\n",
    "                train_fold = pd.concat(classes['train'])\n",
    "\n",
    "\n",
    "                X_train = train_fold[df.columns[:-1]]\n",
    "                y_train = train_fold[target_column]\n",
    "\n",
    "                X_test = test_fold[df.columns[:-1]]\n",
    "                y_test = test_fold[target_column]\n",
    "\n",
    "                \n",
    "                model = model.fit(X_train,y_train)\n",
    "                pred_train = model.predict(X_train)\n",
    "                pred_test = model.predict(X_test)\n",
    "\n",
    "                score_treino = loss_function(y_train,pred_train)\n",
    "                score_teste = loss_function(y_test,pred_test)\n",
    "\n",
    "                modelos.append(model)\n",
    "\n",
    "                folds['score_teste'].append(score_teste)\n",
    "                folds['score_treino'].append(score_treino)\n",
    "                folds['melhor_modelo'] = modelos[np.argmax(folds['score_teste'])]\n",
    "\n",
    "                folds['media_treino'] = np.mean(folds['score_treino'])\n",
    "                folds['media_teste'] = np.mean(folds['score_teste'])\n",
    "\n",
    "        return folds\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe_line = ML_pipeline(2,train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  4.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score_treino': [1.0, 0.9988571428571429],\n",
       " 'score_teste': [0.9862857142857143, 0.9948571428571429],\n",
       " 'melhor_modelo': RandomForestClassifier(),\n",
       " 'media_treino': 0.9994285714285714,\n",
       " 'media_teste': 0.9905714285714287}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pipe_line.k_fold(RandomForestClassifier(),accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  4.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score_treino': [0.9988571428571429, 1.0],\n",
       " 'score_teste': [0.9902857142857143, 0.992],\n",
       " 'melhor_modelo': RandomForestClassifier(),\n",
       " 'media_treino': 0.9994285714285714,\n",
       " 'media_teste': 0.9911428571428571}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pipe_line.stratified_kfold(RandomForestClassifier(),accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3500/3500 [16:48<00:00,  3.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score_treino': [0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9997142040583024,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9991426121749071,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9991426121749071,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9991426121749071,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9991426121749071,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9991426121749071,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9991426121749071,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9991426121749071,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9991426121749071,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9991426121749071,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  0.9994284081166047,\n",
       "  ...],\n",
       " 'score_teste': [1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  ...],\n",
       " 'melhor_modelo': RandomForestClassifier(),\n",
       " 'media_treino': 0.9994266116849714,\n",
       " 'media_teste': 0.9902857142857143}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pipe_line.leave_one_out_kfold(RandomForestClassifier(),accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score_treino': [0.9991181657848325, 0.9991181657848325, 1.0, 1.0],\n",
       " 'score_teste': [0.8863636363636364,\n",
       "  0.9253246753246753,\n",
       "  0.8562610229276896,\n",
       "  0.8597883597883598],\n",
       " 'melhor_modelo': RandomForestClassifier(),\n",
       " 'media_treino': 0.9995590828924162,\n",
       " 'media_teste': 0.8819344236010902}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pipe_line.group_kfold('sex',RandomForestClassifier(),accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score_treino': [0.9982378854625551, 0.9991189427312775, 1.0, 1.0],\n",
       " 'score_teste': [0.9189627228525121,\n",
       "  0.9189627228525121,\n",
       "  0.8387665198237886,\n",
       "  0.8960352422907489],\n",
       " 'melhor_modelo': RandomForestClassifier(),\n",
       " 'media_treino': 0.9993392070484581,\n",
       " 'media_teste': 0.8931818019548905}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pipe_line.groupstrat_kfold('sex',RandomForestClassifier(),accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
